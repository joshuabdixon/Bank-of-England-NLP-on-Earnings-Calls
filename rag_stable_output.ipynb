{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\genna\\Documents\\Cam-Course\\BoE_RAGS\\Bank_of_England_NLP\n"
     ]
    }
   ],
   "source": [
    "##Needed only when run localy --- update using path to your project folder \n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#Update path HERE - dont forget the r in the begining  \n",
    "project_path = Path(r\"C:\\Users\\genna\\Documents\\Cam-Course\\BoE_RAGS\\Bank_of_England_NLP\")\n",
    "os.chdir(project_path)\n",
    "sys.path.insert(0, str(project_path))\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Basic Setup\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 2: Set up OpenAI credentials and cashing\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Caching Embeddings for Stability\n",
    "embedding_cache = {}\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "import openai\n",
    "\n",
    "# ✅ Caching directory\n",
    "CACHE_DIR = \"embedding_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve embedding from OpenAI API or cache to avoid recomputation.\n",
    "    \"\"\"\n",
    "    # Convert text into a hashable key using SHA256\n",
    "    text_key = hashlib.sha256(text.encode()).hexdigest()\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"{text_key}.pkl\")\n",
    "\n",
    "    # Check if embedding exists in cache\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    # Updated OpenAI API call (v1.0+ format)\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=[text]  # Must be a list of strings\n",
    "    )\n",
    "\n",
    "    embedding = response.data[0].embedding  # ✅ Access embedding correctly\n",
    "\n",
    "    # Convert to NumPy array and normalize\n",
    "    embedding = np.array(embedding, dtype=np.float32)\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "\n",
    "    # Save to cache\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(embedding, f)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 212 Q/A segments from: 1_data_and_preprocess/1.0_raw/processed_QnA_correct_pages\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Q&A Documents\n",
    "def load_qna_documents(folder_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Loads all JSON files in folder_path, flattens the Q&A items\n",
    "    into a list of docs. Each doc is a dict with keys:\n",
    "        'text': the question/answer text\n",
    "        'metadata': any extra info (Type, Person, Pages, Filename, etc.)\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".json\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            first_list = data[0]\n",
    "\n",
    "            for qa_obj in first_list:\n",
    "                txt = qa_obj.get(\"Text\", \"\")\n",
    "                topics_list = [list(topic.keys())[0] for topic in qa_obj.get(\"Topics\", [])]\n",
    "                sentiment_key = list(qa_obj.get(\"Sentiment\", {}).keys())[0] if qa_obj.get(\"Sentiment\", {}) else None\n",
    "                \n",
    "                meta = {\n",
    "                    \"Type\": qa_obj.get(\"Type\", \"\"),\n",
    "                    \"Person\": qa_obj.get(\"Person\", \"\"),\n",
    "                    \"Role\": qa_obj.get(\"Role\", \"\"),\n",
    "                    \"Affiliation\": qa_obj.get(\"Affiliation\", \"\"),\n",
    "                    \"Page\": qa_obj.get(\"Page\", []),\n",
    "                    \"Filename\": filename,\n",
    "                    \"Topics\": topics_list,\n",
    "                    \"Sentiment\": sentiment_key\n",
    "                }\n",
    "                \n",
    "                doc = {\n",
    "                    \"text\": txt,\n",
    "                    \"metadata\": meta\n",
    "                }\n",
    "                all_docs.append(doc)\n",
    "    \n",
    "    print(f\"Loaded {len(all_docs)} Q/A segments from: {folder_path}\")\n",
    "    return all_docs\n",
    "\n",
    "# Set folder path\n",
    "folder_path = \"1_data_and_preprocess/1.0_raw/processed_QnA_correct_pages\"\n",
    "docs = load_qna_documents(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: FAISS Index Class\n",
    "\n",
    "class FaissIndex:\n",
    "    def __init__(self, embedding_dim=1536):\n",
    "        \"\"\"\n",
    "        Initializes a FAISS index with normalized embeddings.\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)  # ✅ Use L2 distance for better stability\n",
    "        self.documents = []  # Store document references\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def add_embeddings(self, embeddings: np.ndarray, docs: List[Dict]):\n",
    "        \"\"\"\n",
    "        Add document embeddings + references to FAISS.\n",
    "        \"\"\"\n",
    "        if not self.index.is_trained:\n",
    "            raise ValueError(\"Index is not trained. IndexFlatL2 should be trained by default.\")\n",
    "\n",
    "        self.index.add(embeddings)\n",
    "        self.documents.extend(docs)\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, embeddings])\n",
    "\n",
    "    def rebuild_index(self):\n",
    "        \"\"\"\n",
    "        Rebuild FAISS index to ensure consistency after updates.\n",
    "        \"\"\"\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        self.index.add(self.embeddings)\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k=3):\n",
    "        \"\"\"\n",
    "        Perform FAISS search using cached and normalized embeddings.\n",
    "        \"\"\"\n",
    "        if not isinstance(query_embedding, np.ndarray):\n",
    "            raise TypeError(f\"Expected numpy.ndarray but got {type(query_embedding)}\")\n",
    "\n",
    "        query_embedding_2d = np.expand_dims(query_embedding, axis=0)\n",
    "        scores, indices = self.index.search(query_embedding_2d, top_k)\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            score = float(scores[0][i])\n",
    "            results.append((doc, score))\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: RAG Pipeline with dynamic top_k\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, openai_api_key: str, embedding_dim=1536, top_k=3):\n",
    "        \"\"\"\n",
    "        openai_api_key: Your OpenAI API key\n",
    "        embedding_dim: Must match the embedding model size (1536 for ada-002)\n",
    "        top_k: Number of top results to retrieve (default = 3)\n",
    "        \"\"\"\n",
    "        openai.api_key = openai_api_key\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.top_k = top_k  # ✅ Store top_k as an instance variable\n",
    "        self.index = FaissIndex(embedding_dim=embedding_dim)\n",
    "\n",
    "    def preprocess_document(self, doc):\n",
    "        \"\"\"\n",
    "        Combine metadata and text into a single searchable string.\n",
    "        This allows FAISS to retrieve results based on metadata and text together.\n",
    "        \"\"\"\n",
    "        metadata_text = f\"Person: {doc['metadata'].get('Person', 'Unknown')}. \" \\\n",
    "                        f\"Role: {doc['metadata'].get('Role', 'Unknown')}. \" \\\n",
    "                        f\"Affiliation: {doc['metadata'].get('Affiliation', 'Unknown')}. \" \\\n",
    "                        f\"Topics: {', '.join(doc['metadata'].get('Topics', []))}. \" \\\n",
    "                        f\"Sentiment: {doc['metadata'].get('Sentiment', '0.0')}.\"\n",
    "        \n",
    "        return f\"{metadata_text} {doc['text']}\"  # Append metadata to text\n",
    "\n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get 1536-dim embedding using OpenAI's latest embedding API.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = openai.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=[text]  # Input must be a list of strings\n",
    "            )\n",
    "            embedding = response.data[0].embedding  # Access the embedding array\n",
    "            return np.array(embedding, dtype=np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)  # Return zero-vector in case of error\n",
    "\n",
    "    def build_index(self, all_docs: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        1) Embed each doc (including metadata)\n",
    "        2) Store embeddings + docs in FAISS\n",
    "        \"\"\"\n",
    "        embeddings_list = []\n",
    "        processed_docs = []  # Store processed documents\n",
    "\n",
    "        for doc in all_docs:\n",
    "            processed_text = self.preprocess_document(doc)  \n",
    "            emb = self.embed_text(processed_text)  # Create embedding for processed text\n",
    "            embeddings_list.append(emb)\n",
    "\n",
    "            # Store the processed text as well (for debugging or retrieval)\n",
    "            doc[\"processed_text\"] = processed_text\n",
    "            processed_docs.append(doc)\n",
    "\n",
    "        embeddings_array = np.vstack(embeddings_list)\n",
    "        self.index.add_embeddings(embeddings_array, processed_docs)\n",
    "        print(f\"Index built with {len(all_docs)} documents.\")\n",
    "\n",
    "    def retrieve_topk(self, query: str, top_k=None):\n",
    "        \"\"\"\n",
    "        1) Embed the user query\n",
    "        2) Find top_k docs\n",
    "        \"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k  # Use default top_k if not provided\n",
    "\n",
    "        # ✅ Ensure query is embedded before search\n",
    "        query_emb = self.embed_text(query)\n",
    "\n",
    "        if not isinstance(query_emb, np.ndarray):\n",
    "            raise TypeError(f\"Expected numpy.ndarray for query embedding but got {type(query_emb)}\")\n",
    "\n",
    "        results = self.index.search(query_emb, top_k=top_k)\n",
    "        return results\n",
    "\n",
    "\n",
    "    def _chat_completion(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper to call OpenAI ChatCompletion.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = openai.chat.completions.create(  \n",
    "                model=\"gpt-4o-mini\",  # or \"gpt-4o\"\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=4000\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in ChatCompletion: {e}\")\n",
    "            return \"I'm sorry, I couldn't process your request.\"\n",
    "\n",
    "    def answer_query(self, query: str, top_k=None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        1) Retrieve top_k doc chunks\n",
    "        2) Feed them into ChatCompletion\n",
    "        3) Return the final answer\n",
    "        \"\"\"\n",
    "        retrieved = self.retrieve_topk(query, top_k=top_k)\n",
    "        \n",
    "        # Build a context from top_k docs\n",
    "        context_snippets = []\n",
    "        for (doc, score) in retrieved:\n",
    "            snippet = f\"Text: {doc['text']}\\nMetadata: {doc['metadata']}\\nScore: {score}\\n---\\n\"\n",
    "            context_snippets.append(snippet)\n",
    "        \n",
    "        combined_context = \"\\n\".join(context_snippets)\n",
    "        \n",
    "        # Construct final prompt\n",
    "        system_prompt = (\n",
    "            \"You are a financial analyst helping the PRA, Bank of England.\"\n",
    "            \"Your role is to provide accurate, concise, and well-supported insights.\"\n",
    "            \"If the context does not contain enough information, state that explicitly.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"CONTEXT:\\n{combined_context}\\n\\n\"\n",
    "            f\"USER QUESTION: {query}\\n\\n\"\n",
    "            \"Please give me a concise answer using the context above:\"\n",
    "        )\n",
    "        \n",
    "        final_answer = self._chat_completion(system_prompt, user_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"top_docs\": retrieved,\n",
    "            \"combined_context\": combined_context,\n",
    "            \"answer\": final_answer\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 212 Q/A segments from: 1_data_and_preprocess/1.0_raw/processed_QnA_correct_pages\n",
      "Loaded 212 documents.\n",
      "Index built with 212 documents.\n"
     ]
    }
   ],
   "source": [
    "# Putting It All Together (Demo)\n",
    "\n",
    "# 1) Your folder path containing JSON QnA\n",
    "folder_path = \"1_data_and_preprocess/1.0_raw/processed_QnA_correct_pages\"\n",
    "\n",
    "# 2) Load data\n",
    "docs = load_qna_documents(folder_path)\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# 3) Create pipeline (uses OpenAI key)\n",
    "rag = RAGPipeline(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), embedding_dim=1536, top_k=5)\n",
    "\n",
    "# 4) Build FAISS index\n",
    "rag.build_index(docs)\n",
    "\n",
    "# Initialize FAISS Index\n",
    "faiss_index = FaissIndex()\n",
    "\n",
    "# Generate and add embeddings to FAISS\n",
    "document_embeddings = np.array([get_embedding(doc[\"text\"]) for doc in docs])\n",
    "faiss_index.add_embeddings(document_embeddings, docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Page Numbers</th>\n",
       "      <th>Specific Risk Terms Mentioned</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary of the Discussion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Q2 2024</td>\n",
       "      <td>5, 6</td>\n",
       "      <td>stress capital buffer, capital management</td>\n",
       "      <td>Yeah. So, I'm not going to comment about any c...</td>\n",
       "      <td>Discussion on the volatility and challenges of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Bank AG</td>\n",
       "      <td>Q3 2024</td>\n",
       "      <td>19, 20</td>\n",
       "      <td>capital plan, operational risk</td>\n",
       "      <td>Thanks, Piers. No, not much in the way of new ...</td>\n",
       "      <td>Discussion on capital estimates and operationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JPMorgan Chase</td>\n",
       "      <td>Q4 2024</td>\n",
       "      <td>11</td>\n",
       "      <td>credit quality, economic risks</td>\n",
       "      <td>I would just point the biggest driver of credi...</td>\n",
       "      <td>Discussion on the impact of unemployment on cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Bank Name Time Period Page Numbers  \\\n",
       "0  JPMorgan Chase & Co.     Q2 2024         5, 6   \n",
       "1      Deutsche Bank AG     Q3 2024       19, 20   \n",
       "2        JPMorgan Chase     Q4 2024           11   \n",
       "\n",
       "               Specific Risk Terms Mentioned  \\\n",
       "0  stress capital buffer, capital management   \n",
       "1             capital plan, operational risk   \n",
       "2             credit quality, economic risks   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Yeah. So, I'm not going to comment about any c...   \n",
       "1  Thanks, Piers. No, not much in the way of new ...   \n",
       "2  I would just point the biggest driver of credi...   \n",
       "\n",
       "                           Summary of the Discussion  \n",
       "0  Discussion on the volatility and challenges of...  \n",
       "1  Discussion on capital estimates and operationa...  \n",
       "2  Discussion on the impact of unemployment on cr...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import IPython.display as display\n",
    "\n",
    "# Define user query\n",
    "user_query = \"\"\"\n",
    "Identify discussions by bank executives related to financial stability, capital adequacy, or liquidity. Specifically, look for mentions of key risk terms, including: 'liquidity crunch,' 'funding stress,' 'capital shortfall,' 'Common Equity Tier 1 (CET1) capital,' 'regulatory capital buffer,' 'countercyclical capital buffer,' 'credit risk exposure,' 'loan loss provisions,' 'sovereign risk,' 'interest rate risk,' 'economic downturn,' 'recession impact,' 'market volatility,' 'wholesale funding risk,' 'counterparty risk,' 'systemic risk,' 'stress test results,' or 'macroprudential risk.' \n",
    "\n",
    "Return results in **CSV format** with the following columns:\n",
    "1. **Bank Name** (Extracted from the filename)\n",
    "2. **Time Period** (Extracted from the filename)\n",
    "3. **Page Numbers** (Comma-separated if multiple)\n",
    "4. **Specific Risk Terms Mentioned** (Comma-separated)\n",
    "5. **Text** (Extracted discussion)\n",
    "6. **Summary of the Discussion** (Concise summary of risk discussion)\n",
    "\n",
    "Ensure that the response **strictly follows CSV formatting** with a header row and values properly enclosed in double quotes if necessary.\n",
    "\"\"\"\n",
    "\n",
    "# Get response from RAG model\n",
    "response = rag.answer_query(user_query, top_k=10)\n",
    "\n",
    "# Extract the CSV content\n",
    "file_content = response[\"answer\"]\n",
    "\n",
    "# Preprocess CSV: Remove Markdown artifacts\n",
    "if file_content.startswith(\"```csv\") and file_content.endswith(\"```\"):\n",
    "    file_content = file_content[7:-3].strip()\n",
    "\n",
    "# Convert CSV string to Pandas DataFrame\n",
    "df = pd.read_csv(StringIO(file_content))\n",
    "\n",
    "# Display DataFrame nicely in Jupyter Notebook\n",
    "display.display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Page Numbers</th>\n",
       "      <th>Specific Risk Terms Mentioned</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary of the Discussion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deutsche Bank AG</td>\n",
       "      <td>Q3-2024</td>\n",
       "      <td>19, 20</td>\n",
       "      <td>Basel IV Guidance, Leverage Finance Reviews</td>\n",
       "      <td>Thanks, Piers. No, not much in the way of new ...</td>\n",
       "      <td>Discussion on Basel IV guidance and leverage f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Bank AG</td>\n",
       "      <td>Q3-2024</td>\n",
       "      <td>20</td>\n",
       "      <td>Leverage Finance Reviews</td>\n",
       "      <td>Well, we should only say that we are still wai...</td>\n",
       "      <td>Discussion on the status of leverage finance r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>Q3-2024</td>\n",
       "      <td>11</td>\n",
       "      <td>Regulatory Justification, Economic Impact</td>\n",
       "      <td>We just want the numbers to be done right and ...</td>\n",
       "      <td>Emphasis on the need for careful regulatory ad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Bank Name Time Period Page Numbers  \\\n",
       "0      Deutsche Bank AG     Q3-2024       19, 20   \n",
       "1      Deutsche Bank AG     Q3-2024           20   \n",
       "2  JPMorgan Chase & Co.     Q3-2024           11   \n",
       "\n",
       "                 Specific Risk Terms Mentioned  \\\n",
       "0  Basel IV Guidance, Leverage Finance Reviews   \n",
       "1                     Leverage Finance Reviews   \n",
       "2    Regulatory Justification, Economic Impact   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Thanks, Piers. No, not much in the way of new ...   \n",
       "1  Well, we should only say that we are still wai...   \n",
       "2  We just want the numbers to be done right and ...   \n",
       "\n",
       "                           Summary of the Discussion  \n",
       "0  Discussion on Basel IV guidance and leverage f...  \n",
       "1  Discussion on the status of leverage finance r...  \n",
       "2  Emphasis on the need for careful regulatory ad...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import IPython.display as display\n",
    "\n",
    "# Define user query\n",
    "user_query = \"\"\"\n",
    "Identify discussions by bank executives that would be most interesting to the PRA, Bank of England. \n",
    "\n",
    "Return results in **CSV format** with the following columns:\n",
    "1. **Bank Name** (Extracted from the filename)\n",
    "2. **Time Period** (Extracted from the filename)\n",
    "3. **Page Numbers** (Comma-separated if multiple)\n",
    "4. **Specific Risk Terms Mentioned** (Comma-separated)\n",
    "5. **Text** (Extracted discussion)\n",
    "6. **Summary of the Discussion** (Concise summary of risk discussion)\n",
    "\n",
    "Ensure that the response **strictly follows CSV formatting** with a header row and values properly enclosed in double quotes if necessary.\n",
    "\"\"\"\n",
    "\n",
    "# Get response from RAG model\n",
    "response = rag.answer_query(user_query, top_k=10)\n",
    "\n",
    "# Extract the CSV content\n",
    "file_content = response[\"answer\"]\n",
    "\n",
    "# Preprocess CSV: Remove Markdown artifacts\n",
    "if file_content.startswith(\"```csv\") and file_content.endswith(\"```\"):\n",
    "    file_content = file_content[7:-3].strip()\n",
    "\n",
    "# Convert CSV string to Pandas DataFrame\n",
    "df = pd.read_csv(StringIO(file_content))\n",
    "\n",
    "# Display DataFrame nicely in Jupyter Notebook\n",
    "display.display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
