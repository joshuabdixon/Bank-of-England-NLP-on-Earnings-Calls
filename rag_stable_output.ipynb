{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\joshu\\OneDrive\\Documents\\1 Work\\Bank of England NLP\\Bank-of-England-NLP-on-Earnings-Calls\n"
     ]
    }
   ],
   "source": [
    "# Pathways\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically set the project path to the current working directory\n",
    "project_path = Path.cwd()\n",
    "os.chdir(project_path)\n",
    "sys.path.insert(0, str(project_path))\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Basic Setup\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 2: Set up OpenAI credentials and cashing\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Caching Embeddings for Stability\n",
    "embedding_cache = {}\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "import openai\n",
    "\n",
    "# ✅ Caching directory\n",
    "CACHE_DIR = \"embedding_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve embedding from OpenAI API or cache to avoid recomputation.\n",
    "    \"\"\"\n",
    "    # Convert text into a hashable key using SHA256\n",
    "    text_key = hashlib.sha256(text.encode()).hexdigest()\n",
    "    cache_path = os.path.join(CACHE_DIR, f\"{text_key}.pkl\")\n",
    "\n",
    "    # Check if embedding exists in cache\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    # Updated OpenAI API call (v1.0+ format)\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=[text]  # Must be a list of strings\n",
    "    )\n",
    "\n",
    "    embedding = response.data[0].embedding  # ✅ Access embedding correctly\n",
    "\n",
    "    # Convert to NumPy array and normalize\n",
    "    embedding = np.array(embedding, dtype=np.float32)\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "\n",
    "    # Save to cache\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(embedding, f)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qna_documents(folder_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Loads all JSON files in folder_path, flattens the Q&A items\n",
    "    into a list of docs. Each doc is a dict with keys:\n",
    "        'text': the question/answer text\n",
    "        'metadata': any extra info (Type, Person, Pages, Filename, etc.)\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".json\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Instead of taking data[0], iterate directly over data\n",
    "            for qa_obj in data:\n",
    "                txt = qa_obj.get(\"Text\", \"\")\n",
    "                topics_list = [list(topic.keys())[0] for topic in qa_obj.get(\"Topics\", [])]\n",
    "                sentiment_key = list(qa_obj.get(\"Sentiment\", {}).keys())[0] if qa_obj.get(\"Sentiment\", {}) else None\n",
    "                \n",
    "                meta = {\n",
    "                    \"Type\": qa_obj.get(\"Type\", \"\"),\n",
    "                    \"Person\": qa_obj.get(\"Person\", \"\"),\n",
    "                    \"Role\": qa_obj.get(\"Role\", \"\"),\n",
    "                    \"Affiliation\": qa_obj.get(\"Affiliation\", \"\"),\n",
    "                    \"Page\": qa_obj.get(\"Page\", []),\n",
    "                    \"Filename\": filename,\n",
    "                    \"Topics\": topics_list,\n",
    "                    \"Sentiment\": sentiment_key\n",
    "                }\n",
    "                \n",
    "                doc = {\n",
    "                    \"text\": txt,\n",
    "                    \"metadata\": meta\n",
    "                }\n",
    "                all_docs.append(doc)\n",
    "    \n",
    "    print(f\"Loaded {len(all_docs)} Q/A segments from: {folder_path}\")\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: FAISS Index Class\n",
    "\n",
    "class FaissIndex:\n",
    "    def __init__(self, embedding_dim=1536):\n",
    "        \"\"\"\n",
    "        Initializes a FAISS index with normalized embeddings.\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)  # ✅ Use L2 distance for better stability\n",
    "        self.documents = []  # Store document references\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def add_embeddings(self, embeddings: np.ndarray, docs: List[Dict]):\n",
    "        \"\"\"\n",
    "        Add document embeddings + references to FAISS.\n",
    "        \"\"\"\n",
    "        if not self.index.is_trained:\n",
    "            raise ValueError(\"Index is not trained. IndexFlatL2 should be trained by default.\")\n",
    "\n",
    "        self.index.add(embeddings)\n",
    "        self.documents.extend(docs)\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, embeddings])\n",
    "\n",
    "    def rebuild_index(self):\n",
    "        \"\"\"\n",
    "        Rebuild FAISS index to ensure consistency after updates.\n",
    "        \"\"\"\n",
    "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
    "        self.index.add(self.embeddings)\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k=3):\n",
    "        \"\"\"\n",
    "        Perform FAISS search using cached and normalized embeddings.\n",
    "        \"\"\"\n",
    "        if not isinstance(query_embedding, np.ndarray):\n",
    "            raise TypeError(f\"Expected numpy.ndarray but got {type(query_embedding)}\")\n",
    "\n",
    "        query_embedding_2d = np.expand_dims(query_embedding, axis=0)\n",
    "        scores, indices = self.index.search(query_embedding_2d, top_k)\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            doc = self.documents[idx]\n",
    "            score = float(scores[0][i])\n",
    "            results.append((doc, score))\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: RAG Pipeline with dynamic top_k\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, openai_api_key: str, embedding_dim=1536, top_k=3):\n",
    "        \"\"\"\n",
    "        openai_api_key: Your OpenAI API key\n",
    "        embedding_dim: Must match the embedding model size (1536 for ada-002)\n",
    "        top_k: Number of top results to retrieve (default = 3)\n",
    "        \"\"\"\n",
    "        openai.api_key = openai_api_key\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.top_k = top_k  # ✅ Store top_k as an instance variable\n",
    "        self.index = FaissIndex(embedding_dim=embedding_dim)\n",
    "\n",
    "    def preprocess_document(self, doc):\n",
    "        \"\"\"\n",
    "        Combine metadata and text into a single searchable string.\n",
    "        This allows FAISS to retrieve results based on metadata and text together.\n",
    "        \"\"\"\n",
    "        metadata_text = f\"Person: {doc['metadata'].get('Person', 'Unknown')}. \" \\\n",
    "                        f\"Role: {doc['metadata'].get('Role', 'Unknown')}. \" \\\n",
    "                        f\"Affiliation: {doc['metadata'].get('Affiliation', 'Unknown')}. \" \\\n",
    "                        f\"Topics: {', '.join(doc['metadata'].get('Topics', []))}. \" \\\n",
    "                        f\"Sentiment: {doc['metadata'].get('Sentiment', '0.0')}.\"\n",
    "        \n",
    "        return f\"{metadata_text} {doc['text']}\"  # Append metadata to text\n",
    "\n",
    "    def embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get 1536-dim embedding using OpenAI's latest embedding API.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = openai.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=[text]  # Input must be a list of strings\n",
    "            )\n",
    "            embedding = response.data[0].embedding  # Access the embedding array\n",
    "            return np.array(embedding, dtype=np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)  # Return zero-vector in case of error\n",
    "\n",
    "    def build_index(self, all_docs: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        1) Embed each doc (including metadata)\n",
    "        2) Store embeddings + docs in FAISS\n",
    "        \"\"\"\n",
    "        embeddings_list = []\n",
    "        processed_docs = []  # Store processed documents\n",
    "\n",
    "        for doc in all_docs:\n",
    "            processed_text = self.preprocess_document(doc)  \n",
    "            emb = self.embed_text(processed_text)  # Create embedding for processed text\n",
    "            embeddings_list.append(emb)\n",
    "\n",
    "            # Store the processed text as well (for debugging or retrieval)\n",
    "            doc[\"processed_text\"] = processed_text\n",
    "            processed_docs.append(doc)\n",
    "\n",
    "        embeddings_array = np.vstack(embeddings_list)\n",
    "        self.index.add_embeddings(embeddings_array, processed_docs)\n",
    "        print(f\"Index built with {len(all_docs)} documents.\")\n",
    "\n",
    "    def retrieve_topk(self, query: str, top_k=None):\n",
    "        \"\"\"\n",
    "        1) Embed the user query\n",
    "        2) Find top_k docs\n",
    "        \"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.top_k  # Use default top_k if not provided\n",
    "\n",
    "        # ✅ Ensure query is embedded before search\n",
    "        query_emb = self.embed_text(query)\n",
    "\n",
    "        if not isinstance(query_emb, np.ndarray):\n",
    "            raise TypeError(f\"Expected numpy.ndarray for query embedding but got {type(query_emb)}\")\n",
    "\n",
    "        results = self.index.search(query_emb, top_k=top_k)\n",
    "        return results\n",
    "\n",
    "\n",
    "    def _chat_completion(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Helper to call OpenAI ChatCompletion.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = openai.chat.completions.create(  \n",
    "                model=\"gpt-4o-mini\",  # or \"gpt-4o\"\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=4000\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in ChatCompletion: {e}\")\n",
    "            return \"I'm sorry, I couldn't process your request.\"\n",
    "\n",
    "    def answer_query(self, query: str, top_k=None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        1) Retrieve top_k doc chunks\n",
    "        2) Feed them into ChatCompletion\n",
    "        3) Return the final answer\n",
    "        \"\"\"\n",
    "        retrieved = self.retrieve_topk(query, top_k=top_k)\n",
    "        \n",
    "        # Build a context from top_k docs\n",
    "        context_snippets = []\n",
    "        for (doc, score) in retrieved:\n",
    "            snippet = f\"Text: {doc['text']}\\nMetadata: {doc['metadata']}\\nScore: {score}\\n---\\n\"\n",
    "            context_snippets.append(snippet)\n",
    "        \n",
    "        combined_context = \"\\n\".join(context_snippets)\n",
    "        \n",
    "        # Construct final prompt\n",
    "        system_prompt = (\n",
    "            \"You are a financial analyst helping the PRA, Bank of England.\"\n",
    "            \"Your role is to provide accurate, concise, and well-supported insights.\"\n",
    "            \"If the context does not contain enough information, state that explicitly.\"\n",
    "        )\n",
    "        user_prompt = (\n",
    "            f\"CONTEXT:\\n{combined_context}\\n\\n\"\n",
    "            f\"USER QUESTION: {query}\\n\\n\"\n",
    "            \"Please give me a concise answer using the context above:\"\n",
    "        )\n",
    "        \n",
    "        final_answer = self._chat_completion(system_prompt, user_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"top_docs\": retrieved,\n",
    "            \"combined_context\": combined_context,\n",
    "            \"answer\": final_answer\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 154 Q/A segments from: 4_processed_json_correct_pages\n",
      "Loaded 154 documents.\n",
      "Index built with 154 documents.\n"
     ]
    }
   ],
   "source": [
    "# Putting It All Together (Demo)\n",
    "\n",
    "# 1) Your folder path containing JSON QnA\n",
    "folder_path = \"4_processed_json_correct_pages\"\n",
    "\n",
    "# 2) Load data\n",
    "docs = load_qna_documents(folder_path)\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# 3) Create pipeline (uses OpenAI key)\n",
    "rag = RAGPipeline(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), embedding_dim=1536, top_k=5)\n",
    "\n",
    "# 4) Build FAISS index\n",
    "rag.build_index(docs)\n",
    "\n",
    "# Initialize FAISS Index\n",
    "faiss_index = FaissIndex()\n",
    "\n",
    "# Generate and add embeddings to FAISS\n",
    "document_embeddings = np.array([get_embedding(doc[\"text\"]) for doc in docs])\n",
    "faiss_index.add_embeddings(document_embeddings, docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Page Numbers</th>\n",
       "      <th>Specific Risk Terms Mentioned</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary of the Discussion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>Q1 2024</td>\n",
       "      <td>30, 31</td>\n",
       "      <td>liquidity crunch, funding stress, capital shor...</td>\n",
       "      <td>Thanks, Andrew. So €8 million is the number th...</td>\n",
       "      <td>Discussion highlights the impact of FDIC charg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Bank Name Time Period Page Numbers  \\\n",
       "0  Deutsche Bank     Q1 2024       30, 31   \n",
       "\n",
       "                       Specific Risk Terms Mentioned  \\\n",
       "0  liquidity crunch, funding stress, capital shor...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Thanks, Andrew. So €8 million is the number th...   \n",
       "\n",
       "                           Summary of the Discussion  \n",
       "0  Discussion highlights the impact of FDIC charg...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import IPython.display as display\n",
    "\n",
    "# Define user query\n",
    "user_query = \"\"\"\n",
    "Identify discussions by bank executives related to financial stability, capital adequacy, or liquidity. Specifically, look for mentions of key risk terms, including: 'liquidity crunch,' 'funding stress,' 'capital shortfall,' 'Common Equity Tier 1 (CET1) capital,' 'regulatory capital buffer,' 'countercyclical capital buffer,' 'credit risk exposure,' 'loan loss provisions,' 'sovereign risk,' 'interest rate risk,' 'economic downturn,' 'recession impact,' 'market volatility,' 'wholesale funding risk,' 'counterparty risk,' 'systemic risk,' 'stress test results,' or 'macroprudential risk.' \n",
    "\n",
    "Return results in **CSV format** with the following columns:\n",
    "1. **Bank Name** (Extracted from the filename)\n",
    "2. **Time Period** (Extracted from the filename)\n",
    "3. **Page Numbers** (Comma-separated if multiple)\n",
    "4. **Specific Risk Terms Mentioned** (Comma-separated)\n",
    "5. **Text** (Extracted discussion)\n",
    "6. **Summary of the Discussion** (Concise summary of risk discussion)\n",
    "\n",
    "Ensure that the response **strictly follows CSV formatting** with a header row and values properly enclosed in double quotes if necessary.\n",
    "\"\"\"\n",
    "\n",
    "# Get response from RAG model\n",
    "response = rag.answer_query(user_query, top_k=10)\n",
    "\n",
    "# Extract the CSV content\n",
    "file_content = response[\"answer\"]\n",
    "\n",
    "# Preprocess CSV: Remove Markdown artifacts\n",
    "if file_content.startswith(\"```csv\") and file_content.endswith(\"```\"):\n",
    "    file_content = file_content[7:-3].strip()\n",
    "\n",
    "# Convert CSV string to Pandas DataFrame\n",
    "df = pd.read_csv(StringIO(file_content))\n",
    "\n",
    "# Display DataFrame nicely in Jupyter Notebook\n",
    "display.display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank Name</th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Page Numbers</th>\n",
       "      <th>Specific Risk Terms Mentioned</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary of the Discussion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPMorgan</td>\n",
       "      <td>Q1 2024</td>\n",
       "      <td>8,11</td>\n",
       "      <td>economic health, rate cuts, customer feedback</td>\n",
       "      <td>Jeremy, when you think about the outlook for t...</td>\n",
       "      <td>Discussion on the economic outlook and custome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>Q2 2024</td>\n",
       "      <td>15,32</td>\n",
       "      <td>capital requirements, leveraged finance, conso...</td>\n",
       "      <td>And then second, could you give us an update o...</td>\n",
       "      <td>Inquiry about capital requirements and the out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>Q3 2024</td>\n",
       "      <td>32</td>\n",
       "      <td>consolidation, optionality</td>\n",
       "      <td>And then the second question, I just wanted to...</td>\n",
       "      <td>Discussion on M&amp;A opportunities and risks asso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Bank Name Time Period Page Numbers  \\\n",
       "0       JPMorgan     Q1 2024         8,11   \n",
       "1  Deutsche Bank     Q2 2024        15,32   \n",
       "2  Deutsche Bank     Q3 2024           32   \n",
       "\n",
       "                       Specific Risk Terms Mentioned  \\\n",
       "0      economic health, rate cuts, customer feedback   \n",
       "1  capital requirements, leveraged finance, conso...   \n",
       "2                         consolidation, optionality   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Jeremy, when you think about the outlook for t...   \n",
       "1  And then second, could you give us an update o...   \n",
       "2  And then the second question, I just wanted to...   \n",
       "\n",
       "                           Summary of the Discussion  \n",
       "0  Discussion on the economic outlook and custome...  \n",
       "1  Inquiry about capital requirements and the out...  \n",
       "2  Discussion on M&A opportunities and risks asso...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import IPython.display as display\n",
    "\n",
    "# Define user query\n",
    "user_query = \"\"\"\n",
    "Identify discussions by bank executives that would be most interesting to the PRA, Bank of England. \n",
    "\n",
    "Return results in **CSV format** with the following columns:\n",
    "1. **Bank Name** (Extracted from the filename)\n",
    "2. **Time Period** (Extracted from the filename)\n",
    "3. **Page Numbers** (Comma-separated if multiple)\n",
    "4. **Specific Risk Terms Mentioned** (Comma-separated)\n",
    "5. **Text** (Extracted discussion)\n",
    "6. **Summary of the Discussion** (Concise summary of risk discussion)\n",
    "\n",
    "Ensure that the response **strictly follows CSV formatting** with a header row and values properly enclosed in double quotes if necessary.\n",
    "\"\"\"\n",
    "\n",
    "# Get response from RAG model\n",
    "response = rag.answer_query(user_query, top_k=10)\n",
    "\n",
    "# Extract the CSV content\n",
    "file_content = response[\"answer\"]\n",
    "\n",
    "# Preprocess CSV: Remove Markdown artifacts\n",
    "if file_content.startswith(\"```csv\") and file_content.endswith(\"```\"):\n",
    "    file_content = file_content[7:-3].strip()\n",
    "\n",
    "# Convert CSV string to Pandas DataFrame\n",
    "df = pd.read_csv(StringIO(file_content))\n",
    "\n",
    "# Display DataFrame nicely in Jupyter Notebook\n",
    "display.display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bank_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
